**Current Resources**: Introduction to Statistical Learning, IBM ML / Deep Learning AI, Sebastian Raschka ML.

**Process**

- Using IBM + Sebastian Raschka as Video Content
- Using An Introduction to Statistical Learning as Book Content
- Selectively learn what you need based on this README.md

Then for each alg or topic (when applicable):

1. Learn the Theory via resources
2. Compute Practical Implementations in NumPy from Scratch

**Supervised Learning**
- [X] **Train-Test Split**
  - [X] Regular Splitting
  - [X] Shuffle Splitting
  - [X] Stratified Splitting 
- [X] **Regression**
    - [X] Linear regression
    - [X] Polynomial regression
- [ ] **Classification**
    - [X] Logistic regression
    - [ ] Decision trees
    - [X] K-nearest neighbors
    - [ ] **Class Imbalance**: Understanding and addressing class imbalance issues in classification problems
- [ ] **Support Vector Machines (SVMs)**
    - [ ] Linear
    - [ ] Non-linear SVMs
    - [ ] Kernel methods

**Unsupervised Learning**
- [ ] **Clustering**
    - [ ] K-means
    - [ ] Hierarchical clustering
- [ ] **Dimensionality Reduction**:
    - [ ] PCA
    - [ ] t-SNE

**Model Evaluation**
- [ ] **Evaluation Metrics**
    - [X] Accuracy
    - [X] Precision
    - [x] Recall
    - [X] F1-score
    - [ ] ROC-AUC
- [X] **K-fold Cross-Validation**
- [X] **Log Likelihood**: Understanding log likelihood and its role in model evaluation
- [ ] Understanding how to choose the appropriate evaluation methodologies
- [X] **Model Selection Techniques**: Cross-validation

**Hyperparameter Tuning / Model Selection**
- [ ] **Hyperparameter Tuning**:
  - [X] Grid search
  - [X] Random search
  - [ ] Bayesian optimization, etc.

**Bias-Variance Tradeoff**
- [X] Understanding bias and variance in models
- [X] Techniques to manage bias and variance

**Ensemble Methods**
- [ ] **Bagging**: Random forests
- [ ] **Boosting**: 
  - [ ] AdaBoost
  - [ ] Gradient Boosting

**Introduction to Reinforcement Learning**
- [ ] **Basics**: Agents, environments, rewards, policies

---

**Advanced**

- [ ]  **Advanced Supervised Learning**
    - [ ]  Ensemble Methods: Stacking, blending, and other ensemble techniques.
    - [ ]  Advanced Decision Trees: XGBoost, LightGBM, and CatBoost.
    - [ ]  Advanced Classification Techniques: Naive Bayes, Quadratic Discriminant Analysis (QDA), and Support Vector Classification (SVC), Naive Bayes, QDA.
- [ ]  **Advanced Unsupervised Learning**
    - [ ]  Advanced Clustering: DBSCAN, OPTICS, Mean-shift, and Spectral clustering.
    - [ ]  Advanced Dimensionality Reduction: Non-negative Matrix Factorization (NMF), Independent Component Analysis (ICA), and Autoencoders (without deep learning).
- [ ]  **Model Evaluation and Selection**
    - [ ]  Advanced Evaluation Metrics: Cohen's Kappa, Matthews Correlation Coefficient (MCC), Area Under the Precision-Recall Curve (AUPRC), and Confusion Matrix.
    - [ ]  Model Selection Techniques: AIC, BIC, and other information criteria.
    - [ ]  Advanced Hyperparameter Tuning: Hyperopt, Optuna, and other optimization libraries.
- [ ]  **Advanced Feature Engineering**
    - [ ]  Feature Interaction: Polynomial features, interaction terms, and feature crosses.
- [ ]  **Advanced Model Interpretability**
    - [ ]  Model-agnostic methods: Partial Dependence Plots (PDP), Individual Conditional Expectation (ICE), and SHAP values.
    - [ ]  Model-specific methods: Feature importances, permutation importance, and LIME.
- [ ]  **Advanced Reinforcement Learning**
    - [ ]  Markov Decision Processes (MDPs): Value iteration, policy iteration, and Q-learning.
    - [ ]  Advanced RL Algorithms: Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Monte Carlo Tree Search (MCTS).
    - [ ]  Meta-Reinforcement Learning: Techniques and algorithms that enable agents to learn how to learn, adapting their learning strategies across different tasks or environments.
- [ ]  **Advanced Topics in Machine Learning**
    - [ ]  Imbalanced Classification: Sampling techniques, cost-sensitive learning, and ensemble methods.
    - [ ]  Multi-label Classification: Binary relevance, classifier chains, and label powerset.
    - [ ]  Multi-output Regression: Stacking, regression trees, and neural networks (without deep learning).
    - [ ]  Time Series Analysis: ARIMA, SARIMA, and Prophet.
    - [ ]  Transfer Learning: Model adaptation, domain adaptation, and multi-task learning.
    - [ ]  Online Learning: Stochastic Gradient Descent (SGD), Perceptron, and Passive-Aggressive algorithms.
    - [ ]  Semi-supervised Learning: Self-training, multi-view training, and co-training.
    - [ ]  Active Learning: Pool-based sampling, stream-based sampling, and query synthesis.
    - [ ]  Causal Inference: Potential outcomes framework, propensity score matching, and instrumental variables.
    - [ ]  Fairness, Accountability, and Transparency (FAT) in Machine Learning: Bias mitigation, explainability, and privacy-preserving techniques.

- Projects?
  - Maybe a couple want to get to DL sooner, though would include using ML algs on a dataset, then computing statistics for them (i.e., f1 score, precision, recall, p value etc.). Of course, if ican go for cooler projects i'd go for them.